{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAP5610 HW4 - Yuan Du\n",
    "\n",
    "Code stored at my Github: https://github.com/YuanEldaif/CAP5610\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: \n",
    "### Q1: What is the margin and support vectors? \n",
    "Margin is the distance/width from the decision boudary to the closest data point.\n",
    "Support vectors are the points when the margin is maximized in which the hyperplane that has the largest perpendicular distance between the hyperplane and the closest samples on either side.\n",
    "\n",
    "### Q2: How does SVM deal with non-separable data? \n",
    "By using kernal tricks/functions.\n",
    "\n",
    "### Q3: What is a kernel? \n",
    "A kernel is a mathematical function that maps data points to higher dimensional space to linearly seperate non-linearly separable data. \n",
    "\n",
    "### Q4: How does a kernel relate to feature vectors?\n",
    "A kernel specifies the inner product under some feature mapping to a feature vector in high dimentional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: \n",
    "### Construct a support vector machine that computes the kernel function. Use four values of +1 and -1for both inputs and outputs: \n",
    "### Map the input [x1, x2] into a space consisting of x1 and x1x2. Draw the four input points in this space, and the maximal margin separator. What is the margin?\n",
    "\n",
    "The examples $[x_1,x_2]$ maps to $[x_1, x_1x_2]$ as below:\n",
    "\n",
    "$[-1,-1]$ (negative) maps to $[-1,1]$\n",
    "\n",
    "$[-1,1]$ (positive) maps to $[-1,-1]$\n",
    "\n",
    "$[1,-1]$ (positive) maps to $[1,-1]$\n",
    "\n",
    "$[1,1]$ (negative) maps to $[1,1]$\n",
    "\n",
    "Thus, the positive examples have $x_1x_2$ = −1 and the negative examples have $x_1x_2$ = +1.The drawings is shown in a seperate pdf file \"CAP5610 HW4 Task 2\". The maximimal margin separator is the line $x_1x_2$ = 0, with a margin of 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: \n",
    "### Recall that the equation of the circle in the 2-dimensional plane is $(x1 - a)^2 + (x2 - b)^2 - r^2 = 0$. Please expand out the formula and show that every circular region is linearly separable from the rest of the plane in the feature space (x1, x2, x12, x22).\n",
    "The circle equation expands into below terms: \n",
    "\n",
    "$x_1^2 + x_2^2 - 2ax_1 - 2bx_2 + (a^2 +b^2 -r^2) = 0$\n",
    "\n",
    "Thus, weights in the feature space :w = (2a, 2b, 1, 1) and intercept $(a^2 +b^2 -r^2)$. This shows that a circular boundary is linear in this feature space, allowing linear separability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: \n",
    "### Recall that the equation of an ellipse in the 2-dimensional plane is $c(x1 - a)^2 + d(x2 - b)^2 - 1 = 0$. Please show that an SVM using the polynomial kernel of degree 2, $K(u, v) = (1 + u . v)^2$, is equivalent to a linear SVM in the feature space (1, x1, x2, x12, x22, x1x2) and hence that SVMs with this kernel can separate any elliptic region from the rest of the plane.\n",
    "The ellipse equation expands into below terms:\n",
    "$cx_1^2 + dx_2^2 - 2acx_1 - 2bdx_2 + (ca^2 +db^2 -1) = 0$\n",
    "\n",
    "Thus, weights in the feature space :w = (1, 2a, 2b, c, d, 0) and intercept $(ca^2 +db^2 -1)$. This shows that an elliptical boundary is linear in this feature space, allowing linear separability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: \n",
    "### (a) Plot these six training points. Are the classes {+, −} linearly separable?\n",
    "Yes. The classes are linearly separable\n",
    "\n",
    "### (b) Construct the weight vector of the maximum margin hyperplane by inspection and identify the support vectors.\n",
    "The maximum margin hyperplane should have a slope of −1 and should satisfy x1 = 3/2, x2 = 0. Thus, the equation is $x_1 +x_2 = 3/2$ and the weight vector is $(1,1)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: \n",
    "### Consider a dataset with 3 points in 1-D:\n",
    "### (a) Are the classes {+, -} linearly separable?\n",
    "No. The classes are not linearly separable.\n",
    "\n",
    "### (b) Consider mapping each point to 3-D using new feature vectors ?x) = [1, sqrt(2)x, x^2 ] . Are the classes now linearly separable? If so, find a separating hyperplane.\n",
    "Yes. They are now linearly separable. The points are mapped to $(1, 0, 0),(1, \\sqrt{2},1),(1,-\\sqrt{2},1)$ respectively. A separating hyperplane is given by the weight vector (0,0,1) in the new space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7:\n",
    "### Learning SVMs on the Titanic dataset. Please report your five-fold cross validation classification accuracies on Titanic training set, with respect to the linear, quadratic, and RBF kernels. Which kernel is the best in your case?\n",
    "\n",
    "With default SVM setting, linear kernel is the best. After parameter tuning, rbf kernel is the best as shown in below table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Load in the train and test datasets\n",
    "train = pd.read_csv('C:/Work/Project/CAP5610/HW/HW1/train.csv')\n",
    "test = pd.read_csv('C:/Work/Project/CAP5610/HW/HW1/test.csv')\n",
    "\n",
    "combine = [train, test]\n",
    "\n",
    "# Store our passenger ID for easy access\n",
    "PassengerId = test['PassengerId']\n",
    "\n",
    "#Q14 drop ticket\n",
    "#Q15 drop cabin\n",
    "train = train.drop(['Ticket', 'Cabin'], axis=1)\n",
    "test = test.drop(['Ticket', 'Cabin'], axis=1)\n",
    "combine = [train, test]\n",
    "\n",
    "#Q16 Convert sex to numeric\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n",
    "\n",
    "#Q17 \n",
    "# fill missing values with median column values\n",
    "train.fillna(train.median(), inplace=True)\n",
    "test.fillna(test.median(), inplace=True)\n",
    "\n",
    "#Q18 Fill embarked with mode\n",
    "freq_port = train.Embarked.dropna().mode()[0]\n",
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n",
    "    \n",
    "#Q19 complete Fare using mode\n",
    "test['Fare'].fillna(test['Fare'].dropna().median(), inplace=True)    \n",
    "\n",
    "# update combine\n",
    "combine = [train, test]\n",
    "\n",
    "#Q20 Convert the Fare feature to ordinal values\n",
    "for dataset in combine:\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "#Convert categorical variables into numeric\n",
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "\n",
    "#Drop Name,PassengerId\n",
    "train = train.drop(['Name','PassengerId'], axis=1)\n",
    "test = test.drop(['Name','PassengerId'], axis=1)\n",
    "\n",
    "X=train.drop(columns=['Survived','SibSp','Parch','Embarked']).values\n",
    "y=train['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Scale data\n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(X)\n",
    "#scaler.fit(test)\n",
    "train = scaler.transform(X)\n",
    "#test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 4), (712,), (179, 4), (179,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#Model selection and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear\n",
    "clf_svm_lin = SVC(kernel='linear')\n",
    "#quadratic\n",
    "clf_svm_qua = SVC(kernel='poly', degree=2)\n",
    "#RBF\n",
    "clf_svm_rbf = SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning parameters for each SVM kernels\n",
    "\n",
    "#a) Select Parameters for different kernels\n",
    "#parameters = {'kernel':['linear','poly', 'rbf'],'C':[0.1, 1, 5, 10]}\n",
    "kernel = ['linear','poly', 'rbf']\n",
    "C = [50, 10, 1.0, 0.1, 0.01]\n",
    "gamma = ['scale'] #default\n",
    "\n",
    "svc_grid_lin = dict(kernel=['linear'],C=C,gamma=gamma)\n",
    "svc_grid_qua = dict(kernel=['poly'],C=C,gamma=gamma)\n",
    "svc_grid_rbf = dict(kernel=['rbf'],C=C,gamma=gamma)\n",
    "grids = [svc_grid_lin,svc_grid_qua,svc_grid_rbf]\n",
    "\n",
    "#b) Decide Metric: Accuracy\n",
    "from sklearn.metrics import make_scorer\n",
    "scorer = make_scorer(metrics.accuracy_score)\n",
    "\n",
    "#c) Create GridSearch Object \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results dataframe\n",
    "cols = ['Model','SVM-linear','SVM-Poly','SVM-rbf']\n",
    "\n",
    "resul = pd.DataFrame(columns=cols)\n",
    "resul.set_index(\"Model\",inplace=True)\n",
    "resul.loc['Default'] = [0,0,0]\n",
    "resul.loc['GridSearch'] = [0,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default results for SVM with different kernels \n",
    "models = [clf_svm_lin,clf_svm_qua,clf_svm_rbf]\n",
    "col = 0\n",
    "for model in models:\n",
    "    model.fit(X_train,y_train)\n",
    "    resul.iloc[0,col] = model.score(X_test,y_test)\n",
    "    col += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM-linear</th>\n",
       "      <th>SVM-Poly</th>\n",
       "      <th>SVM-rbf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Default</th>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.608939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch</th>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.77095</td>\n",
       "      <td>0.793296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SVM-linear  SVM-Poly   SVM-rbf\n",
       "Model                                    \n",
       "Default      0.782123  0.620112  0.608939\n",
       "GridSearch   0.782123   0.77095  0.793296"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary results for SVM with different kernels - default & gridsearch\n",
    "col = 0\n",
    "for ind in range(0,len(models)):\n",
    "    grid_search = GridSearchCV(estimator=models[col], \n",
    "                  param_grid=grids[col], n_jobs=-1, cv=5,  \n",
    "                  scoring=scorer,error_score=0)\n",
    "    grid_clf_acc = grid_search.fit(X_train, y_train)\n",
    "    resul.iloc[1,col] = grid_clf_acc.score(X_test,y_test)\n",
    "    col += 1\n",
    "    \n",
    "resul.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
